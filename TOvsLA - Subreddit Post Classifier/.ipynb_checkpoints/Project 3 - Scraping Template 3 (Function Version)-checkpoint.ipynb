{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "okay, so there isn't a clear real utility for the simple classification of post into a subreddit. For one, 100% of posts on reddit belong to a subreddit so a post would always have its subreddit attached to it.  It's just obviously not very impressive in utility.  At first glance, that is.\n",
    "\n",
    "The process of creating a binary classifier can bring to light some other factors about whatever's being studied. Keeping in mind that the sample of information they're getting from is demographically biased so there are limitations in its generaliztion to many other applications. \n",
    "\n",
    "Learning about what keywords are most important to a city may give you an idea about the culture in the city, or what's important to the city. So many things could be discovered from this sort of exploration. Could also be used in market research analysis (yawn) by comparing one brand's subreddit with another's. Or, one show's fanbase vs another's. This binary classification model can also be modified to be applied to message filters.\n",
    "\n",
    "Sociologists for example, would have a field day with this with all the types of thematic analyses they could apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gabriela Osorio\n",
    "#### DSI Project 3 - Creating a Binary Subreddit Classifier - TO vs. LA\n",
    "#### November 5, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preamble\n",
    ">  Reddit is an online public content sharing platform that is organized into different categories known as subreddits. Subreddits are comprised of user-submitted posts that can be text, media, or both. Posts can be interacted with through user-prompted upvotes, downvotes, comments, and of course, views. This project will outline the creation of a subreddit classifier that predicts the subreddit a given post is from. Specifially, it's a binary classifier for the Toronto and Los Angeles subreddits. This model can then be expanded upon to explore what the most important characteristics are of this classifier.\n",
    "\n",
    "#### Quick Model Summary\n",
    "> **Input**: 'Title' <br>\n",
    "**Output**: Binary label ('LA' or 'TO')<br>\n",
    "**Type**: Binary Classifier: Random Forest, Support Vector Machine <br>\n",
    "**Metrics of Success**: Accuracy <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1: Webscraping Using the Reddit API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by scraping posts from the Toronto and LA subreddits using Reddit's API. This portion was created from a template provided by Max Humber, course instructor, so it should not be mistaken for the author's original work. \n",
    "\n",
    "Potentially interesting and influential features of posts that have been identified and included in this webscraping include: \n",
    "- subreddit: to be part of the target vector later on \n",
    "- title: text input  \n",
    "- selftext : actual text from the post\n",
    "- downs : upvotes, positive points\n",
    "- ups : downvotes, negative points\n",
    "- num_comments : number of comments\n",
    "- permalink \n",
    "- name \n",
    "- author \n",
    "- is_original_content : binary answer to \"Is content in selftext original?\"\n",
    "- edited : binary answer to \"Has this post been edited?\"\n",
    "- media_only : binary answer to \"Does the post only have a photo?\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 1A: Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'My User Agent 1.0'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_page(url, after=''):\n",
    "    params = {'after': after}\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    return response.json()['data']['children']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_post(post):\n",
    "    keep = ['subreddit', 'title', 'selftext', 'downs', 'ups', 'num_comments', 'permalink', 'name', 'author', 'time', 'is_original_content', 'edited', 'media_only'] \n",
    "    return {k:v for k, v in post['data'].items() if k in keep}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_page(page):\n",
    "    after = ''\n",
    "    posts = []\n",
    "    for post in page:\n",
    "        post = parse_post(post)\n",
    "        after = post['name']\n",
    "        posts.append(post)\n",
    "    return posts, after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_posts = []\n",
    "def fetch_subreddit(subreddit, pages=25):\n",
    "    url = f'https://www.reddit.com/r/{subreddit}.json'\n",
    "    after = ''\n",
    "    for i in range(pages):\n",
    "        print(f'Fetching Page {i + 1}')\n",
    "        page = fetch_page(url, after)\n",
    "        posts, after = parse_page(page)\n",
    "        all_posts.extend(posts)\n",
    "        time.sleep(5)\n",
    "    return all_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Page 1\n",
      "Fetching Page 2\n",
      "Fetching Page 3\n",
      "Fetching Page 4\n"
     ]
    }
   ],
   "source": [
    "posts = fetch_subreddit('Toronto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at the fetched goods through a DataFrame aka stopping to smell the roses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(all_posts)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 1A: Creating and Exporting Scraped Goods as CSV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "now = str(datetime.datetime.now())[:19]\n",
    "\n",
    "filename = f'data/datasci scrape {now}.csv'\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TO=pd.read_csv('./data/TO.csv')\n",
    "TO.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LA= pd.read_csv('./data/LA.csv')\n",
    "LA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
